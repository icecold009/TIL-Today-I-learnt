# Big O Notation & Algorithmic Efficiency

Big O notation is a mathematical landscape used to describe the limiting behavior of a function when the argument tends towards a particular value or infinity. In CS, it measures **Time Complexity** (how long it takes) and **Space Complexity** (how much memory it uses).

### ðŸ“ˆ Common Complexities (Ordered by Efficiency)

1.  **$O(1)$ - Constant Time:** The execution time stays the same regardless of input size (e.g., accessing an array index).
2.  **$O(\log n)$ - Logarithmic Time:** The input size is halved each step (e.g., Binary Search).
3.  **$O(n)$ - Linear Time:** Time grows proportionally to input size (e.g., a simple `for` loop).
4.  **$O(n \log n)$ - Linearithmic Time:** The standard for efficient sorting algorithms (e.g., Merge Sort, Quick Sort).
5.  **$O(n^2)$ - Quadratic Time:** Performance is the square of the input (e.g., nested loops, Bubble Sort).

---

### ðŸ§® The Math Behind the Notation

We define Big O by looking at the **growth rate**. If an algorithm has a processing time of $T(n) = 3n^2 + 5n + 10$, we drop the constants and lower-order terms because as $n$ approaches infinity, $n^2$ dominates the total time.

$$T(n) \approx O(n^2)$$



### ðŸ›  Why It Matters in Practice
Imagine you are searching for a name in a phonebook of 1,000,000 people:
* **Linear Search ($O(n)$):** You check every page. In the worst case, you make **1,000,000** comparisons.
* **Binary Search ($O(\log n)$):** You split the book in half each time. You find the name in roughly **20** comparisons.

The difference isn't just a few seconds; it's the difference between a functional app and a crashed server.
